{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "043ef827-73ad-46c9-9981-4da8f878b051",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Top 3 Salaries per Department\n",
    "### You have a dataset: employee_id, name, salary, department. Write a query to get the top 3 highest-paid employees in each department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b542b78-3fc0-4753-b7fd-0ef7ef699b53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------+----------+----+\n|employee_id|   name|salary|department|rank|\n+-----------+-------+------+----------+----+\n|          2|    Bob| 60000|        HR|   1|\n|          3|Charlie| 55000|        HR|   2|\n|          1|  Alice| 50000|        HR|   3|\n|          6|  Frank| 90000|        IT|   1|\n|          5|    Eve| 85000|        IT|   2|\n|          7|  Grace| 75000|        IT|   3|\n|         11|   Kara| 49000|     Sales|   1|\n|          9|    Ivy| 47000|     Sales|   2|\n|         10|   Jack| 46000|     Sales|   3|\n+-----------+-------+------+----------+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, dense_rank\n",
    "\n",
    "# Step 2: Sample employee data\n",
    "data = [11212222222222222222222222222222222222222222222222222222222222222222222222aq                                                                qww\n",
    "    (1, \"Alice\", 50000, \"HR\"),\n",
    "    (2, \"Bob\", 60000, \"HR\"),\n",
    "    (3, \"Charlie\", 55000, \"HR\"),\n",
    "    (4, \"David\", 70000, \"IT\"),\n",
    "    (5, \"Eve\", 85000, \"IT\"),\n",
    "    (6, \"Frank\", 90000, \"IT\"),\n",
    "    (7, \"Grace\", 75000, \"IT\"),\n",
    "    (8, \"Hank\", 45000, \"Sales\"),\n",
    "    (9, \"Ivy\", 47000, \"Sales\"),\n",
    "    (10, \"Jack\", 46000, \"Sales\"),\n",
    "    (11, \"Kara\", 49000, \"Sales\")\n",
    "]\n",
    "\n",
    "columns = [\"employee_id\", \"name\", \"salary\", \"department\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Step 3: Define window partitioned by department and ordered by descending salary\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "# Step 4: Apply dense_rank to assign rank within each department\n",
    "ranked_df = df.withColumn(\"rank\", dense_rank().over(windowSpec))\n",
    "\n",
    "# Step 5: Filter top 3 salaries per department\n",
    "top3_df = ranked_df.filter(col(\"rank\") <= 3)\n",
    "\n",
    "# Step 6: Show result\n",
    "top3_df.select(\"employee_id\", \"name\", \"salary\", \"department\", \"rank\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba13d5f3-676c-423c-86b6-11655eb2c0c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Handling Salary Ties with rank()\n",
    "### In the same employee dataset, you want to rank employees by salary within each department, and account for ties (equal salaries should get the same rank, and next rank should skip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b3a2fbf-78cd-41b7-997a-75a67499d8d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------+----------+----+\n|employee_id|   name|salary|department|rank|\n+-----------+-------+------+----------+----+\n|          2|    Bob| 60000|        HR|   1|\n|          3|Charlie| 60000|        HR|   1|\n|          1|  Alice| 50000|        HR|   3|\n|          6|  Frank| 90000|        IT|   1|\n|          5|    Eve| 85000|        IT|   2|\n|          7|  Grace| 85000|        IT|   2|\n|          4|  David| 70000|        IT|   4|\n|          9|    Ivy| 47000|     Sales|   1|\n|         11|   Kara| 47000|     Sales|   1|\n|         10|   Jack| 46000|     Sales|   3|\n|          8|   Hank| 45000|     Sales|   4|\n+-----------+-------+------+----------+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, rank\n",
    "\n",
    "# Step 2: Sample employee data with salary ties\n",
    "data = [\n",
    "    (1, \"Alice\", 50000, \"HR\"),\n",
    "    (2, \"Bob\", 60000, \"HR\"),\n",
    "    (3, \"Charlie\", 60000, \"HR\"),\n",
    "    (4, \"David\", 70000, \"IT\"),\n",
    "    (5, \"Eve\", 85000, \"IT\"),\n",
    "    (6, \"Frank\", 90000, \"IT\"),\n",
    "    (7, \"Grace\", 85000, \"IT\"),\n",
    "    (8, \"Hank\", 45000, \"Sales\"),\n",
    "    (9, \"Ivy\", 47000, \"Sales\"),\n",
    "    (10, \"Jack\", 46000, \"Sales\"),\n",
    "    (11, \"Kara\", 47000, \"Sales\")\n",
    "]\n",
    "\n",
    "columns = [\"employee_id\", \"name\", \"salary\", \"department\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Step 3: Define window partitioned by department and ordered by descending salary\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "# Step 4: Apply rank to handle salary ties (same salary gets the same rank, next rank skips)\n",
    "ranked_df = df.withColumn(\"rank\", rank().over(windowSpec))\n",
    "\n",
    "# Step 5: Show the result\n",
    "ranked_df.select(\"employee_id\", \"name\", \"salary\", \"department\", \"rank\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4c411ce-1902-4d59-9d8f-7054268ce5d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.Avoid Gaps in Rank using dense_rank()\n",
    "### If your business needs consecutive ranking even for tied salaries (no rank gaps), how would you adjust the logic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96773e96-13cb-48c7-9c9f-3f3b23eaa0e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------+----------+----+\n|employee_id|   name|salary|department|rank|\n+-----------+-------+------+----------+----+\n|          2|    Bob| 60000|        HR|   1|\n|          3|Charlie| 60000|        HR|   1|\n|          1|  Alice| 50000|        HR|   2|\n|          6|  Frank| 90000|        IT|   1|\n|          5|    Eve| 85000|        IT|   2|\n|          7|  Grace| 85000|        IT|   2|\n|          4|  David| 70000|        IT|   3|\n|          9|    Ivy| 47000|     Sales|   1|\n|         11|   Kara| 47000|     Sales|   1|\n|         10|   Jack| 46000|     Sales|   2|\n|          8|   Hank| 45000|     Sales|   3|\n+-----------+-------+------+----------+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, dense_rank\n",
    "\n",
    "# Step 2: Sample employee data with salary ties\n",
    "data = [\n",
    "    (1, \"Alice\", 50000, \"HR\"),\n",
    "    (2, \"Bob\", 60000, \"HR\"),\n",
    "    (3, \"Charlie\", 60000, \"HR\"),\n",
    "    (4, \"David\", 70000, \"IT\"),\n",
    "    (5, \"Eve\", 85000, \"IT\"),\n",
    "    (6, \"Frank\", 90000, \"IT\"),\n",
    "    (7, \"Grace\", 85000, \"IT\"),\n",
    "    (8, \"Hank\", 45000, \"Sales\"),\n",
    "    (9, \"Ivy\", 47000, \"Sales\"),\n",
    "    (10, \"Jack\", 46000, \"Sales\"),\n",
    "    (11, \"Kara\", 47000, \"Sales\")\n",
    "]\n",
    "\n",
    "columns = [\"employee_id\", \"name\", \"salary\", \"department\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Step 3: Define window partitioned by department and ordered by descending salary\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "# Step 4: Apply dense_rank to avoid gaps in ranks (consecutive ranking even with ties)\n",
    "dense_ranked_df = df.withColumn(\"rank\", dense_rank().over(windowSpec))\n",
    "\n",
    "# Step 5: Show the result\n",
    "dense_ranked_df.select(\"employee_id\", \"name\", \"salary\", \"department\", \"rank\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf681bba-3ece-4b11-b446-4222bf96595c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Find Highest Salary Employee per Department\n",
    "### Find only the highest-paid employee(s) in each department. Use a window function to do it (not groupBy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efb7312d-281d-46e7-b611-4a56d362d9f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------+----------+----+\n|employee_id|   name|salary|department|rank|\n+-----------+-------+------+----------+----+\n|          3|Charlie| 70000|        HR|   1|\n|          7|  Grace| 95000|        IT|   1|\n|         11|   Kara| 50000|     Sales|   1|\n+-----------+-------+------+----------+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, rank\n",
    "\n",
    "# Step 2: Sample employee data\n",
    "data = [\n",
    "    (1, \"Alice\", 50000, \"HR\"),\n",
    "    (2, \"Bob\", 60000, \"HR\"),\n",
    "    (3, \"Charlie\", 70000, \"HR\"),\n",
    "    (4, \"David\", 90000, \"IT\"),\n",
    "    (5, \"Eve\", 85000, \"IT\"),\n",
    "    (6, \"Frank\", 90000, \"IT\"),\n",
    "    (7, \"Grace\", 95000, \"IT\"),\n",
    "    (8, \"Hank\", 45000, \"Sales\"),\n",
    "    (9, \"Ivy\", 47000, \"Sales\"),\n",
    "    (10, \"Jack\", 46000, \"Sales\"),\n",
    "    (11, \"Kara\", 50000, \"Sales\")\n",
    "]\n",
    "\n",
    "columns = [\"employee_id\", \"name\", \"salary\", \"department\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Step 3: Define window specification partitioned by department and ordered by salary descending\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n",
    "\n",
    "# Step 4: Apply rank function to rank employees by salary within each department\n",
    "ranked_df = df.withColumn(\"rank\", rank().over(windowSpec))\n",
    "\n",
    "# Step 5: Filter to get only the highest-paid employee(s) (rank = 1)\n",
    "highest_paid_df = ranked_df.filter(col(\"rank\") == 1)\n",
    "\n",
    "# Step 6: Show the result\n",
    "highest_paid_df.select(\"employee_id\", \"name\", \"salary\", \"department\", \"rank\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2f38e4b-efe8-4e7f-aa77-988bbece2369",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Nth Highest Salary Across Company\n",
    "### You want to find the employee with the 3rd highest salary in the entire company, even if there are ties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3989626b-84f2-4f0f-94ef-b07837600757",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+------+----------+----+\n|employee_id|   name|salary|department|rank|\n+-----------+-------+------+----------+----+\n|          3|Charlie| 70000|        HR|   3|\n+-----------+-------+------+----------+----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, dense_rank\n",
    "\n",
    "# Step 2: Sample employee data (includes salary ties)\n",
    "data = [\n",
    "    (1, \"Alice\", 50000, \"HR\"),\n",
    "    (2, \"Bob\", 60000, \"HR\"),\n",
    "    (3, \"Charlie\", 70000, \"HR\"),\n",
    "    (4, \"David\", 90000, \"IT\"),\n",
    "    (5, \"Eve\", 85000, \"IT\"),\n",
    "    (6, \"Frank\", 90000, \"IT\"),\n",
    "    (7, \"Grace\", 85000, \"IT\"),\n",
    "    (8, \"Hank\", 45000, \"Sales\"),\n",
    "    (9, \"Ivy\", 47000, \"Sales\"),\n",
    "    (10, \"Jack\", 46000, \"Sales\"),\n",
    "    (11, \"Kara\", 50000, \"Sales\")\n",
    "]\n",
    "\n",
    "columns = [\"employee_id\", \"name\", \"salary\", \"department\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Step 3: Define a window across the entire company (no partition), ordered by salary descending\n",
    "windowSpec = Window.orderBy(col(\"salary\").desc())\n",
    "\n",
    "# Step 4: Apply dense_rank (avoids skipping rank numbers for ties)\n",
    "ranked_df = df.withColumn(\"rank\", dense_rank().over(windowSpec))\n",
    "\n",
    "# Step 5: Filter where rank == 3 (3rd highest salary)\n",
    "third_highest_df = ranked_df.filter(col(\"rank\") == 3)\n",
    "\n",
    "# Step 6: Show result\n",
    "third_highest_df.select(\"employee_id\", \"name\", \"salary\", \"department\", \"rank\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d51a7a44-ff2c-489d-a7f8-412b7e9aaa46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. Detect Duplicate Records\n",
    "### You suspect some employees have duplicate entries. Use row_number() on columns employee_id, name, department to detect and filter duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc85a86a-a1d1-4bde-b2ff-677447970998",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+------+----------+\n|employee_id|name|salary|department|\n+-----------+----+------+----------+\n|          2| Bob| 60000|        HR|\n|          5| Eve| 85000|        IT|\n+-----------+----+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, row_number\n",
    "\n",
    "# Step 2: Sample data with duplicates\n",
    "data = [\n",
    "    (1, \"Alice\", 50000, \"HR\"),\n",
    "    (2, \"Bob\", 60000, \"HR\"),\n",
    "    (3, \"Charlie\", 70000, \"HR\"),\n",
    "    (4, \"David\", 90000, \"IT\"),\n",
    "    (5, \"Eve\", 85000, \"IT\"),\n",
    "    (6, \"Frank\", 90000, \"IT\"),\n",
    "    (7, \"Grace\", 85000, \"IT\"),\n",
    "    (2, \"Bob\", 60000, \"HR\"),       # duplicate\n",
    "    (5, \"Eve\", 85000, \"IT\"),       # duplicate\n",
    "    (8, \"Hank\", 45000, \"Sales\"),\n",
    "    (9, \"Ivy\", 47000, \"Sales\"),\n",
    "    (10, \"Jack\", 46000, \"Sales\"),\n",
    "    (11, \"Kara\", 50000, \"Sales\")\n",
    "]\n",
    "\n",
    "columns = [\"employee_id\", \"name\", \"salary\", \"department\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Step 3: Define window over duplicates based on employee_id, name, department\n",
    "windowSpec = Window.partitionBy(\"employee_id\", \"name\", \"department\").orderBy(\"salary\")\n",
    "\n",
    "# Step 4: Apply row_number() to detect duplicates\n",
    "numbered_df = df.withColumn(\"row_number\", row_number().over(windowSpec))\n",
    "\n",
    "# Step 5: Filter duplicates (row_number > 1)\n",
    "duplicates_df = numbered_df.filter(col(\"row_number\") > 1)\n",
    "\n",
    "# Step 6: Show duplicate rows\n",
    "duplicates_df.select(\"employee_id\", \"name\", \"salary\", \"department\").show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Scenario-Based-Questions on Rank,Row_Number,Dense_Rank",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}